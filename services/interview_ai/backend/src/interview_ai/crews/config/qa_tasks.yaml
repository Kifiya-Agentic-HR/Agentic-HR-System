question_generation_task:
  description: >
    Generate a warm, friendly, and personalized response that includes one new question for the candidate. 
    Use the candidate’s user info and role info to tailor your language naturally, avoiding generic phrasing or abrupt transitions. 
    Acknowledge the candidate’s previous answer briefly if needed, but do not over-reference it or repeat details extensively. 
    Choose a single skill from the provided skills dictionary that has fewer than 3 questions asked so far. If more than one skill meets this criterion, select the skill with the fewest questions asked. 
    Formulate one direct, conversational question about that skill. 
    Above all, sound human and relevant without being awkward or too formal.

    IMPORTANT:
    - Return the received skills dictionary exactly as it is (verbatim) from the Answer Evaluation Task. 
    - Do not modify skill names, remove, or add any fields to the skills dictionary in your output.

    Reference for you:
      User Info:
        {user_info}
      Job Title:
        {role_info}
      Recent Conversation History:
        {conversation_history}

  expected_output: >
    An "ongoing" state response containing a personalized question for the candidate.

    FORMAT RULES:
    1. Strict JSON compliance. 
    2. No extra keys or text outside the JSON. 
    3. Maintain the "skills" field as received—verbatim—from the Answer Evaluation Task.

    Example output structure:
    {
      "state": "ongoing",
      "text": "A personalized question for the candidate.",
      "skills": { ... the same dictionary you received ... }
    }

    Do not include any additional text or formatting like "```json".
  
  agent: question_generator


answer_evaluation_task:
  description: >
    Read the user's most recent answer and evaluate how well it addresses each relevant skill. 
    Update the skills dictionary by adjusting the score for any skill that the user’s answer pertains to. 
    Use the following scoring criteria:

    1. 1–3: Poor  
       - The response is off-topic, too brief, or does not demonstrate the skill adequately.
    2. 4–6: Average  
       - The answer includes some relevant points but lacks detail or specificity.
    3. 7–9: Good  
       - The candidate offers detailed and specific evidence of competence in the skill.
    4. 10: Excellent  
       - The response fully demonstrates deep expertise with clear, relevant examples.

    SPECIAL RULE:
    - If the user explicitly states they have no experience with a skill, assign that skill a score of 1–3 and increment the number of questions asked for that skill by 3.

    For every skill that you decide to update:
    - Increment its "number_of_questions" by 1.
    - Update its "score" based on the criteria above.
    - Add or update an "evidence" field containing a concise reason for the assigned score. If there was previous reasoning for this skill, incorporate it briefly but remain succinct.

    IMPORTANT:
    - Do NOT add new skills not in the dictionary. 
    - Return ALL skills in the dictionary, even if some are not updated. 
    - Preserve unmodified fields (e.g., "required_level") exactly as they are. 
    - Keep JSON compliance strict. No additional text or formatting.

    Recent Conversation History:
      {conversation_history}

    User Answer:
      {user_answer}
    
    Skills Dictionary:
      {skills}

  expected_output: >
    Updated skills JSON. 
    FORMAT RULES:
    1. Strict JSON compliance (no backticks, no YAML).
    2. Maintain the exact key structure for each skill. 
    3. Include all original skills, whether updated or not.
    4. For each updated skill, include "score", "required_level", "number_of_questions", and "evidence".

    Example Output:
    {
      "skills": {
        "skill_1": {
          "score": 5,
          "required_level": "...",
          "number_of_questions": 2,
          "evidence": "Concise reason incorporating any previous reasoning here."
        },
        "skill_2": {
          "score": ...,
          "required_level": "...",
          "number_of_questions": ...,
          "evidence": "..."
        },
        ...
      }
    }

  agent: answer_evaluator